{
    "docs": [
        {
            "location": "/",
            "text": "Consider TPOT your \nData Science Assistant\n. TPOT is a Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.\n\n\n\n\n\n\n\n\n\n\n\n\nTPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n\n\n\n\n\n\n\n\nAn example machine learning pipeline\n\n\n\n\n\n\nOnce TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.\n\n\n\n\n\n\n\n\nAn example TPOT pipeline\n\n\n\n\n\n\nTPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.\n\n\nTPOT is still under active development\n and we encourage you to check back on this repository regularly for updates.",
            "title": "Home"
        },
        {
            "location": "/installing/",
            "text": "Installation instructions\n\n\nTPOT is built on top of several existing Python libraries, including:\n\n\n\n\n\n\nNumPy\n\n\n\n\n\n\nSciPy\n\n\n\n\n\n\nscikit-learn\n\n\n\n\n\n\nDEAP\n\n\n\n\n\n\nupdate_checker\n\n\n\n\n\n\ntqdm\n\n\n\n\n\n\nMost of the necessary Python packages can be installed via the \nAnaconda Python distribution\n, which we strongly recommend that you use. We also strongly recommend that you use of Python 3 over Python 2 if you're given the choice.\n\n\nNumPy, SciPy, and scikit-learn can be installed in Anaconda via the command:\n\n\nconda install numpy scipy scikit-learn\n\n\n\n\nDEAP, update_checker, and tqdm (used for verbose TPOT runs) can be installed with \npip\n via the command:\n\n\npip install deap update_checker tqdm\n\n\n\n\nFinally to install TPOT itself, run the following command:\n\n\npip install tpot\n\n\n\n\nPlease \nfile a new issue\n if you run into installation problems.",
            "title": "Installation"
        },
        {
            "location": "/installing/#installation-instructions",
            "text": "TPOT is built on top of several existing Python libraries, including:    NumPy    SciPy    scikit-learn    DEAP    update_checker    tqdm    Most of the necessary Python packages can be installed via the  Anaconda Python distribution , which we strongly recommend that you use. We also strongly recommend that you use of Python 3 over Python 2 if you're given the choice.  NumPy, SciPy, and scikit-learn can be installed in Anaconda via the command:  conda install numpy scipy scikit-learn  DEAP, update_checker, and tqdm (used for verbose TPOT runs) can be installed with  pip  via the command:  pip install deap update_checker tqdm  Finally to install TPOT itself, run the following command:  pip install tpot  Please  file a new issue  if you run into installation problems.",
            "title": "Installation instructions"
        },
        {
            "location": "/using/",
            "text": "TPOT on the command line\n\n\nTo use TPOT via the command line, enter the following command with a path to the data file:\n\n\ntpot /path_to/data_file.csv\n\n\n\n\nTPOT offers several arguments that can be provided at the command line:\n\n\n\n\n\n\nArgument\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\n-is\n\n\nINPUT_SEPARATOR\n\n\nAny string\n\n\nCharacter used to separate columns in the input file.\n\n\n\n\n\n\n-o\n\n\nOUTPUT_FILE\n\n\nString path to a file\n\n\nFile to export the code for the final optimized pipeline.\n\n\n\n\n\n\n-g\n\n\nGENERATIONS\n\n\nAny positive integer\n\n\nNumber of generations to run pipeline optimization over. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize over. TPOT will evaluate GENERATIONS x POPULATION_SIZE number of pipelines in total.\n\n\n\n\n\n\n-p\n\n\nPOPULATION_SIZE\n\n\nAny positive integer\n\n\nNumber of individuals in the GP population. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize over. TPOT will evaluate GENERATIONS x POPULATION_SIZE number of pipelines in total.\n\n\n\n\n\n\n-mr\n\n\nMUTATION_RATE\n\n\n[0.0, 1.0]\n\n\nGP mutation rate. We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms.\n\n\n\n\n\n\n-xr\n\n\nCROSSOVER_RATE\n\n\n[0.0, 1.0]\n\n\nGP crossover rate in the range [0.0, 1.0]. We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms.\n\n\n\n\n\n\n-cv\n\n\nNUM_CV_FOLDS\n\n\n[2, 10]\n\n\nThe number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT pipeline optimization process.\n\n\n\n\n\n\n-scoring\n\n\nSCORING_FN\n\n\n\"accuracy\", \"adjusted_rand_score\", \"average_precision\", \"f1\", \"f1_macro\", \"f1_micro\", \"f1_samples\", \"f1_weighted\", \"log_loss\", \"precision\", \"precision_macro\", \"precision_micro\", \"precision_samples\", \"precision_weighted\", \"r2\", \"recall\", \"recall_macro\", \"recall_micro\", \"recall_samples\", \"recall_weighted\", \"roc_auc\"\n\n\nFunction used to evaluate the goodness of a given pipeline for the classification problem. By default, balanced class accuracy is used. TPOT assumes that this scoring function should be maximized, i.e., higher is better.\n\n\n\n\n\n\n-s\n\n\nRANDOM_STATE\n\n\nAny positive integer\n\n\nRandom number generator seed for reproducibility. Set this seed if you want your TPOT run to be reproducible with the same seed and data set in the future.\n\n\n\n\n\n\n-v\n\n\nVERBOSITY\n\n\n{0,1,2}\n\n\nHow much information TPOT communicates while it is running: 0 = none, 1 = minimal, 2 = all.  A setting of 2 will add a progress bar during the optimization procedure.\n\n\n\n\n\n\n--no-update-check\n\n\nN/A\n\n\nFlag indicating whether the TPOT version checker should be disabled.\n\n\n\n\n\n\n--version\n\n\nN/A\n\n\nShow TPOT's version number and exit.\n\n\n\n\n\n\n--help\n\n\nN/A\n\n\nShow TPOT's help documentation and exit.\n\n\n\n\n\n\n\nAn example command-line call to TPOT may look like:\n\n\ntpot data/mnist.csv -is , -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2\n\n\n\n\nTPOT with code\n\n\nWe've taken care to design the TPOT interface to be as similar as possible to scikit-learn.\n\n\nTPOT can be imported just like any regular Python module. To import TPOT, type:\n\n\nfrom tpot import TPOT\n\n\n\n\nthen create an instance of TPOT as follows:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT()\n\n\n\n\nNote that you can pass several parameters to the TPOT instantiation call:\n\n\n\n\n\n\nParameter\n\n\nValid values\n\n\nEffect\n\n\n\n\n\n\ngeneration\n\n\nAny positive integer\n\n\nThe number of generations to run pipeline optimization over. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize over. TPOT will evaluate generations x population_size number of pipelines in total.\n\n\n\n\n\n\npopulation_size\n\n\nAny positive integer\n\n\nThe number of individuals in the GP population. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize over. TPOT will evaluate generations x population_size number of pipelines in total.\n\n\n\n\n\n\nmutation_rate\n\n\n[0.0, 1.0]\n\n\nThe mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\n\n\n\n\ncrossover_rate\n\n\n[0.0, 1.0]\n\n\nThe crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.\n\n\n\n\n\n\nnum_cv_folds\n\n\n[2, 10]\n\n\nThe number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT pipeline optimization process.\n\n\n\n\n\n\nscoring_function\n\n\n\"accuracy\", \"adjusted_rand_score\", \"average_precision\", \"f1\", \"f1_macro\", \"f1_micro\", \"f1_samples\", \"f1_weighted\", \"log_loss\", \"precision\", \"precision_macro\", \"precision_micro\", \"precision_samples\", \"precision_weighted\", \"r2\", \"recall\", \"recall_macro\", \"recall_micro\", \"recall_samples\", \"recall_weighted\", \"roc_auc\"\n\n\nFunction used to evaluate the goodness of a given pipeline for the classification problem. By default, balanced class accuracy is used. TPOT assumes that this scoring function should be maximized, i.e., higher is better.\n\n\n\n\n\n\nrandom_state\n\n\nAny positive integer\n\n\nThe random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.\n\n\n\n\n\n\nverbosity\n\n\n[0, 1, 2]\n\n\nHow much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all. A setting of 2 will add a progress bar to calls to fit().\n\n\n\n\n\n\ndisable_update_check\n\n\n[True, False]\n\n\nFlag indicating whether the TPOT version checker should be disabled.\n\n\n\n\n\n\n\nSome example code with custom TPOT parameters might look like:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\n\n\n\n\nNow TPOT is ready to optimize a pipeline for you. You can tell TPOT to optimize a pipeline based on a data set with the \nfit\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\n\n\n\n\nThe \nfit()\n function takes in a training data set and uses k-fold cross-validation when evaluating pipelines. It then initializes the genetic programming algoritm to find the best pipeline based on average k-fold score.\n\n\nYou can then proceed to evaluate the final pipeline on the testing set with the \nscore()\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(testing_features, testing_classes))\n\n\n\n\nFinally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the \nexport()\n function:\n\n\nfrom tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(testing_features, testing_classes))\npipeline_optimizer.export('tpot_exported_pipeline.py')\n\n\n\n\nOnce this code finishes running, \ntpot_exported_pipeline.py\n will contain the Python code for the optimized pipeline.\n\n\nCheck our \nexamples\n to see TPOT applied to some specific data sets.",
            "title": "Using TPOT"
        },
        {
            "location": "/using/#tpot-on-the-command-line",
            "text": "To use TPOT via the command line, enter the following command with a path to the data file:  tpot /path_to/data_file.csv  TPOT offers several arguments that can be provided at the command line:    Argument  Parameter  Valid values  Effect    -is  INPUT_SEPARATOR  Any string  Character used to separate columns in the input file.    -o  OUTPUT_FILE  String path to a file  File to export the code for the final optimized pipeline.    -g  GENERATIONS  Any positive integer  Number of generations to run pipeline optimization over. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize over. TPOT will evaluate GENERATIONS x POPULATION_SIZE number of pipelines in total.    -p  POPULATION_SIZE  Any positive integer  Number of individuals in the GP population. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize over. TPOT will evaluate GENERATIONS x POPULATION_SIZE number of pipelines in total.    -mr  MUTATION_RATE  [0.0, 1.0]  GP mutation rate. We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms.    -xr  CROSSOVER_RATE  [0.0, 1.0]  GP crossover rate in the range [0.0, 1.0]. We recommend using the default parameter unless you understand how the crossover rate affects GP algorithms.    -cv  NUM_CV_FOLDS  [2, 10]  The number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT pipeline optimization process.    -scoring  SCORING_FN  \"accuracy\", \"adjusted_rand_score\", \"average_precision\", \"f1\", \"f1_macro\", \"f1_micro\", \"f1_samples\", \"f1_weighted\", \"log_loss\", \"precision\", \"precision_macro\", \"precision_micro\", \"precision_samples\", \"precision_weighted\", \"r2\", \"recall\", \"recall_macro\", \"recall_micro\", \"recall_samples\", \"recall_weighted\", \"roc_auc\"  Function used to evaluate the goodness of a given pipeline for the classification problem. By default, balanced class accuracy is used. TPOT assumes that this scoring function should be maximized, i.e., higher is better.    -s  RANDOM_STATE  Any positive integer  Random number generator seed for reproducibility. Set this seed if you want your TPOT run to be reproducible with the same seed and data set in the future.    -v  VERBOSITY  {0,1,2}  How much information TPOT communicates while it is running: 0 = none, 1 = minimal, 2 = all.  A setting of 2 will add a progress bar during the optimization procedure.    --no-update-check  N/A  Flag indicating whether the TPOT version checker should be disabled.    --version  N/A  Show TPOT's version number and exit.    --help  N/A  Show TPOT's help documentation and exit.    An example command-line call to TPOT may look like:  tpot data/mnist.csv -is , -o tpot_exported_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2",
            "title": "TPOT on the command line"
        },
        {
            "location": "/using/#tpot-with-code",
            "text": "We've taken care to design the TPOT interface to be as similar as possible to scikit-learn.  TPOT can be imported just like any regular Python module. To import TPOT, type:  from tpot import TPOT  then create an instance of TPOT as follows:  from tpot import TPOT\n\npipeline_optimizer = TPOT()  Note that you can pass several parameters to the TPOT instantiation call:    Parameter  Valid values  Effect    generation  Any positive integer  The number of generations to run pipeline optimization over. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize over. TPOT will evaluate generations x population_size number of pipelines in total.    population_size  Any positive integer  The number of individuals in the GP population. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize over. TPOT will evaluate generations x population_size number of pipelines in total.    mutation_rate  [0.0, 1.0]  The mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to apply random changes to every generation. We don't recommend that you tweak this parameter unless you know what you're doing.    crossover_rate  [0.0, 1.0]  The crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We don't recommend that you tweak this parameter unless you know what you're doing.    num_cv_folds  [2, 10]  The number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT pipeline optimization process.    scoring_function  \"accuracy\", \"adjusted_rand_score\", \"average_precision\", \"f1\", \"f1_macro\", \"f1_micro\", \"f1_samples\", \"f1_weighted\", \"log_loss\", \"precision\", \"precision_macro\", \"precision_micro\", \"precision_samples\", \"precision_weighted\", \"r2\", \"recall\", \"recall_macro\", \"recall_micro\", \"recall_samples\", \"recall_weighted\", \"roc_auc\"  Function used to evaluate the goodness of a given pipeline for the classification problem. By default, balanced class accuracy is used. TPOT assumes that this scoring function should be maximized, i.e., higher is better.    random_state  Any positive integer  The random number generator seed for TPOT. Use this to make sure that TPOT will give you the same results each time you run it against the same data set with that seed.    verbosity  [0, 1, 2]  How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = all. A setting of 2 will add a progress bar to calls to fit().    disable_update_check  [True, False]  Flag indicating whether the TPOT version checker should be disabled.    Some example code with custom TPOT parameters might look like:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)  Now TPOT is ready to optimize a pipeline for you. You can tell TPOT to optimize a pipeline based on a data set with the  fit  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)  The  fit()  function takes in a training data set and uses k-fold cross-validation when evaluating pipelines. It then initializes the genetic programming algoritm to find the best pipeline based on average k-fold score.  You can then proceed to evaluate the final pipeline on the testing set with the  score()  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(testing_features, testing_classes))  Finally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the  export()  function:  from tpot import TPOT\n\npipeline_optimizer = TPOT(generations=5, population_size=20, cv=5, random_state=42, verbosity=2)\npipeline_optimizer.fit(training_features, training_classes)\nprint(pipeline_optimizer.score(testing_features, testing_classes))\npipeline_optimizer.export('tpot_exported_pipeline.py')  Once this code finishes running,  tpot_exported_pipeline.py  will contain the Python code for the optimized pipeline.  Check our  examples  to see TPOT applied to some specific data sets.",
            "title": "TPOT with code"
        },
        {
            "location": "/examples/MNIST_Example/",
            "text": "Below is a minimal working example with the practice MNIST data set.\n\n\nfrom tpot import TPOT\nfrom sklearn.datasets import load_digits\nfrom sklearn.cross_validation import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25)\n\ntpot = TPOT(generations=5, population_size=20, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_mnist_pipeline.py')\n\n\n\n\nFor details on how the \nfit()\n, \nscore()\n and \nexport()\n functions work, see the \nusage documentation\n.\n\n\nRunning this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding Python code should be exported to the \ntpot_mnist_pipeline.py\n file and look similar to the following:\n\n\nimport numpy as np\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = np.recfromcsv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')\nfeatures = tpot_data.view((np.float64, len(tpot_data.dtype.names)))\nfeatures = np.delete(features, tpot_data.dtype.names.index('class'), axis=1)\ntraining_features, testing_features, training_classes, testing_classes =     train_test_split(features, tpot_data['class'], random_state=42)\n\nexported_pipeline = make_pipeline(\n    KNeighborsClassifier(n_neighbors=3, weights=\"uniform\")\n)\n\nexported_pipeline.fit(training_features, training_classes)\nresults = exported_pipeline.predict(testing_features)",
            "title": "MNIST Example"
        },
        {
            "location": "/examples/IRIS_Example/",
            "text": "The following code illustrates the usage of TPOT with the IRIS data set.\n\n\nfrom tpot import TPOT\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\n\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),\n    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)\n\ntpot = TPOT(generations=5, population_size=20, verbosity=2)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_iris_pipeline.py')\n\n\n\n\nRunning this code should discover a pipeline that achieves ~96% testing accuracy.\n\n\nFor details on how the \nfit()\n, \nscore()\n and \nexport()\n functions work, see the \nusage documentation\n.\n\n\nAfter running the above code, the corresponding Python code should be exported to the \ntpot_iris_pipeline.py\n file and look similar to the following:\n\n\nimport numpy as np\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import FunctionTransformer, PolynomialFeatures\n\n# NOTE: Make sure that the class is labeled 'class' in the data file\ntpot_data = np.recfromcsv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)\ntraining_features, testing_features, training_classes, testing_classes = \\\n    train_test_split(features, tpot_data['class'], random_state=42)\n\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    LogisticRegression(C=0.9, dual=False, penalty=\"l2\")\n)\n\nexported_pipeline.fit(training_features, training_classes)\nresults = exported_pipeline.predict(testing_features)",
            "title": "IRIS Example"
        },
        {
            "location": "/examples/Titanic_Kaggle_Example/",
            "text": "To see the TPOT applied the Titanic Kaggle dataset, see the Jupyter notebook \nhere\n.",
            "title": "Titanic Kaggle Example"
        },
        {
            "location": "/contributing/",
            "text": "We welcome you to \ncheck the existing issues\n for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please \nfile a new issue\n so we can discuss it.\n\n\nHow to contribute\n\n\nThe preferred way to contribute to TPOT is to fork the \n\nmain repository\n on\nGitHub:\n\n\n\n\n\n\nFork the \nproject repository\n:\n   click on the 'Fork' button near the top of the page. This creates\n   a copy of the code under your account on the GitHub server.\n\n\n\n\n\n\nClone this copy to your local disk:\n\n\n  $ git clone git@github.com:YourLogin/tpot.git\n  $ cd tpot\n\n\n\n\n\n\n\nCreate a branch to hold your changes:\n\n\n  $ git checkout -b my-contribution\n\n\n\n\n\n\n\nMake sure your local environment is setup correctly for development. Installation instructions are almost identical to \nthe user instructions\n except that TPOT should \nnot\n be installed. If you have TPOT installed on your computer then make sure you are using a virtual environment that does not have TPOT installed. Furthermore, you should make sure you have installed the \nnose\n package into your development environment so that you can test changes locally.\n\n\n  $ conda install nose\n\n\n\n\n\n\n\nStart making changes on your newly created branch, remembering to never work on the \nmaster\n branch! Work on this copy on your computer using Git to do the version control.\n\n\n\n\n\n\nOnce some changes are saved locally, you can use your tweaked version of TPOT by navigating to the project's base directory and running TPOT directly from the command line:\n\n\n  $ python -m tpot.tpot\n\n\n\nor by running script that imports and uses the TPOT module with code similar to \nfrom tpot import TPOT\n\n\n\n\n\n\nTo check your changes haven't broken any existing tests and to check new tests you've added pass run the following (note, you must have the \nnose\n package installed within your dev environment for this to work):\n\n\n  $ nosetests -s -v\n\n\n\n\n\n\n\nWhen you're done editing and local testing, run:\n\n\n  $ git add modified_files\n  $ git commit\n\n\n\n\n\n\n\nto record your changes in Git, then push them to GitHub with:\n\n\n      $ git push -u origin my-contribution\n\n\n\nFinally, go to the web page of your fork of the TPOT repo, and click 'Pull Request' (PR) to send your changes to the maintainers for review. Make sure that you send your PR to the \ndevelopment\n branch, as the \nmaster\n branch is reserved for the latest stable release. This will start the CI server to check all the project's unit tests run and send an email to the maintainers.\n\n\n(If any of the above seems like magic to you, then look up the \n\nGit documentation\n on the web.)\n\n\nBefore submitting your pull request\n\n\nBefore you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.\n\n\nIf your contribution changes TPOT in any way:\n\n\n\n\n\n\nUpdate the \ndocumentation\n so all of your changes are reflected there.\n\n\n\n\n\n\nUpdate the \nREADME\n if anything there has changed.\n\n\n\n\n\n\nIf your contribution involves any code changes:\n\n\n\n\n\n\nUpdate the \nproject unit tests\n to test your code changes.\n\n\n\n\n\n\nMake sure that your code is properly commented with \ndocstrings\n and comments explaining your rationale behind non-obvious coding practices.\n\n\n\n\n\n\nIf your code affected any of the pipeline operators, make sure that the corresponding \nexport functionality\n reflects those changes.\n\n\n\n\n\n\nIf your contribution requires a new library dependency:\n\n\n\n\n\n\nDouble-check that the new dependency is easy to install via \npip\n or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep TPOT easy to install.\n\n\n\n\n\n\nAdd the required version of the library to \n.travis.yml\n\n\n\n\n\n\nAdd a line to pip install the library to \n.travis_install.sh\n\n\n\n\n\n\nAdd a line to print the version of the library to \n.travis_install.sh\n\n\n\n\n\n\nSimilarly add a line to print the version of the library to \n.travis_test.sh\n\n\n\n\n\n\nUpdating the documentation\n\n\nWe use \nmkdocs\n to manage our \ndocumentation\n. This allows us to write the docs in Markdown and compile them to HTML as needed. Below are a few useful commands to know when updating the documentation. Make sure that you are running them in the base documentation directory, \ndocs\n.\n\n\n\n\n\n\nmkdocs serve\n: Hosts of a local version of the documentation that you can access at the provided URL. The local version will update automatically as you save changes to the documentation.\n\n\n\n\n\n\nmkdocs build --clean\n: Creates a fresh build of the documentation in HTML. Always run this before deploying the documentation to GitHub.\n\n\n\n\n\n\nmkdocs gh-deploy\n: Deploys the documentation to GitHub. If you're deploying on your fork of TPOT, the online documentation should be accessible at \nhttp://<YOUR GITHUB USERNAME>.github.io/tpot/\n. Generally, you shouldn't need to run this command because you can view your changes with \nmkdocs serve\n.\n\n\n\n\n\n\nAfter submitting your pull request\n\n\nAfter submitting your pull request, \nTravis-CI\n will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.\n\n\nCheck back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#how-to-contribute",
            "text": "The preferred way to contribute to TPOT is to fork the  main repository  on\nGitHub:    Fork the  project repository :\n   click on the 'Fork' button near the top of the page. This creates\n   a copy of the code under your account on the GitHub server.    Clone this copy to your local disk:    $ git clone git@github.com:YourLogin/tpot.git\n  $ cd tpot    Create a branch to hold your changes:    $ git checkout -b my-contribution    Make sure your local environment is setup correctly for development. Installation instructions are almost identical to  the user instructions  except that TPOT should  not  be installed. If you have TPOT installed on your computer then make sure you are using a virtual environment that does not have TPOT installed. Furthermore, you should make sure you have installed the  nose  package into your development environment so that you can test changes locally.    $ conda install nose    Start making changes on your newly created branch, remembering to never work on the  master  branch! Work on this copy on your computer using Git to do the version control.    Once some changes are saved locally, you can use your tweaked version of TPOT by navigating to the project's base directory and running TPOT directly from the command line:    $ python -m tpot.tpot  or by running script that imports and uses the TPOT module with code similar to  from tpot import TPOT    To check your changes haven't broken any existing tests and to check new tests you've added pass run the following (note, you must have the  nose  package installed within your dev environment for this to work):    $ nosetests -s -v    When you're done editing and local testing, run:    $ git add modified_files\n  $ git commit    to record your changes in Git, then push them to GitHub with:        $ git push -u origin my-contribution  Finally, go to the web page of your fork of the TPOT repo, and click 'Pull Request' (PR) to send your changes to the maintainers for review. Make sure that you send your PR to the  development  branch, as the  master  branch is reserved for the latest stable release. This will start the CI server to check all the project's unit tests run and send an email to the maintainers.  (If any of the above seems like magic to you, then look up the  Git documentation  on the web.)",
            "title": "How to contribute"
        },
        {
            "location": "/contributing/#before-submitting-your-pull-request",
            "text": "Before you submit a pull request for your contribution, please work through this checklist to make sure that you have done everything necessary so we can efficiently review and accept your changes.  If your contribution changes TPOT in any way:    Update the  documentation  so all of your changes are reflected there.    Update the  README  if anything there has changed.    If your contribution involves any code changes:    Update the  project unit tests  to test your code changes.    Make sure that your code is properly commented with  docstrings  and comments explaining your rationale behind non-obvious coding practices.    If your code affected any of the pipeline operators, make sure that the corresponding  export functionality  reflects those changes.    If your contribution requires a new library dependency:    Double-check that the new dependency is easy to install via  pip  or Anaconda and supports both Python 2 and 3. If the dependency requires a complicated installation, then we most likely won't merge your changes because we want to keep TPOT easy to install.    Add the required version of the library to  .travis.yml    Add a line to pip install the library to  .travis_install.sh    Add a line to print the version of the library to  .travis_install.sh    Similarly add a line to print the version of the library to  .travis_test.sh",
            "title": "Before submitting your pull request"
        },
        {
            "location": "/contributing/#updating-the-documentation",
            "text": "We use  mkdocs  to manage our  documentation . This allows us to write the docs in Markdown and compile them to HTML as needed. Below are a few useful commands to know when updating the documentation. Make sure that you are running them in the base documentation directory,  docs .    mkdocs serve : Hosts of a local version of the documentation that you can access at the provided URL. The local version will update automatically as you save changes to the documentation.    mkdocs build --clean : Creates a fresh build of the documentation in HTML. Always run this before deploying the documentation to GitHub.    mkdocs gh-deploy : Deploys the documentation to GitHub. If you're deploying on your fork of TPOT, the online documentation should be accessible at  http://<YOUR GITHUB USERNAME>.github.io/tpot/ . Generally, you shouldn't need to run this command because you can view your changes with  mkdocs serve .",
            "title": "Updating the documentation"
        },
        {
            "location": "/contributing/#after-submitting-your-pull-request",
            "text": "After submitting your pull request,  Travis-CI  will automatically run unit tests on your changes and make sure that your updated code builds and runs on Python 2 and 3. We also use services that automatically check code quality and test coverage.  Check back shortly after submitting your pull request to make sure that your code passes these checks. If any of the checks come back with a red X, then do your best to address the errors.",
            "title": "After submitting your pull request"
        },
        {
            "location": "/citing/",
            "text": "If you use TPOT in a scientific publication, please consider citing at least one of the following papers:\n\n\nRandal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). \nAutomating biomedical data science through tree-based pipeline optimization\n. \nApplications of Evolutionary Computation\n, pages 123-137.\n\n\nBibTeX entry:\n\n\n@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007/978-3-319-31204-0_9},\n    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}\n}\n\n\n\n\nEvaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science\n\n\nRandal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). \nEvaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science\n. \nProceedings of GECCO 2016\n, pages 485-492.\n\n\nBibTeX entry:\n\n\n@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http://doi.acm.org/10.1145/2908812.2908918},\n    doi = {10.1145/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n\n\n\n\nAlternatively, you can cite the repository directly with the following DOI:",
            "title": "Citing"
        },
        {
            "location": "/support/",
            "text": "TPOT was developed in the \nComputational Genetics Lab\n with funding from the \nNIH\n. We're incredibly grateful for their support during the development of this project.\n\n\nThe TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.",
            "title": "Support"
        }
    ]
}